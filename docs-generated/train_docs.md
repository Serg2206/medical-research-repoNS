# Documentation: train.py

**Файл:** `train.py`

**Дата:** 2025-11-26

---

# Документация

## Описание функционала

Этот код представляет собой скрипт для обучения модели классификации текста на основе BERT. Он использует библиотеку transformers для загрузки предобученной модели BERT и токенизатора. Данные для обучения и тестирования загружаются из CSV-файла, разделяются на обучающую и тестовую выборки и преобразуются в формат, подходящий для обучения модели BERT.

## Классы и функции

### Класс `MedicalDataset`

Этот класс представляет собой настраиваемый датасет для обучения и тестирования модели. Он принимает в качестве входных данных DataFrame pandas, токенизатор и максимальную длину последовательности.

#### Параметры

- `data`: DataFrame pandas, содержащий текст и метки.
- `tokenizer`: Токенизатор из библиотеки transformers.
- `max_length`: Максимальная длина последовательности.

#### Методы

- `__len__`: Возвращает количество элементов в датасете.
- `__getitem__`: Возвращает токенизированный текст и метку для элемента с указанным индексом.

### Функция `prepare_data`

Эта функция загружает данные из CSV-файла, разделяет их на обучающую и тестовую выборки и сохраняет результат в указанной директории.

#### Параметры

- `input_csv`: Путь к входному CSV-файлу.
- `output_dir`: Директория для сохранения обработанных данных.
- `test_size`: Доля данных, которая будет использоваться для тестирования. По умолчанию равна 0.2.

## Примеры использования

```python
# Подготовка данных
prepare_data("data.csv", "processed_data")

# Загрузка токенизатора
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Загрузка данных
train_data = pd.read_csv("processed_data/train.csv")
test_data = pd.read_csv("processed_data/test.csv")

# Создание датасетов
train_dataset = MedicalDataset(train_data, tokenizer)
test_dataset = MedicalDataset(test_data, tokenizer)

# Создание загрузчиков данных
train_loader = DataLoader(train_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)
```