# Documentation: train.py

**Файл:** `train.py`

**Дата:** 2025-11-10

---

# Документация

## Описание

Этот код представляет собой скрипт для обучения модели классификации текста на основе BERT. Он включает в себя подготовку данных, создание датасета и функцию для обучения модели.

## Классы и функции

### Класс `MedicalDataset`

Этот класс представляет собой настраиваемый датасет для обучения и тестирования модели.

#### Параметры

- `data`: DataFrame, содержащий данные для обучения или тестирования.
- `tokenizer`: Токенизатор, используемый для преобразования текста в токены.
- `max_length`: Максимальная длина последовательности токенов.

#### Методы

- `__len__`: Возвращает количество элементов в датасете.
- `__getitem__`: Возвращает токенизированный текст и метку для указанного индекса.

### Функция `prepare_data`

Эта функция загружает данные из CSV-файла, разделяет их на обучающую и тестовую выборки и сохраняет их в указанной директории.

#### Параметры

- `input_csv`: Путь к входному CSV-файлу.
- `output_dir`: Директория для сохранения обработанных данных.
- `test_size`: Доля данных, которая будет использоваться для тестирования. По умолчанию равна 0.2.

## Пример использования

```python
# Подготовка данных
prepare_data("data.csv", "processed_data")

# Загрузка токенизатора
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Загрузка данных
train_data = pd.read_csv("processed_data/train.csv")
test_data = pd.read_csv("processed_data/test.csv")

# Создание датасетов
train_dataset = MedicalDataset(train_data, tokenizer)
test_dataset = MedicalDataset(test_data, tokenizer)

# Создание загрузчиков данных
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)
```

После этого вы можете использовать `train_loader` и `test_loader` для обучения и тестирования вашей модели соответственно.