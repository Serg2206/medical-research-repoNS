# Documentation: train.py

**Файл:** `train.py`

**Дата:** 2025-11-06

---

# Документация

## Описание функционала

Этот код представляет собой начало процесса обучения модели машинного обучения на основе BERT для классификации текстов. В частности, он включает в себя:

- Импорт необходимых библиотек и модулей
- Настройку логирования
- Определение параметров модели
- Создание класса датасета для обработки и подготовки данных
- Функцию для подготовки данных, которая читает данные из CSV файла, и разделяет их на обучающую и тестовую выборки

## Параметры и возвращаемые значения

### Класс `MedicalDataset`

- `__init__(self, data, tokenizer, max_length=128)`: Инициализирует объект класса. Принимает следующие параметры:
  - `data`: DataFrame, содержащий данные для обучения или тестирования.
  - `tokenizer`: Токенизатор, используемый для преобразования текста в токены.
  - `max_length`: Максимальная длина последовательности токенов. По умолчанию равна 128.

- `__len__(self)`: Возвращает количество записей в данных. Не принимает параметры.

- `__getitem__(self, idx)`: Возвращает токенизированные данные для записи с индексом `idx`. Принимает следующие параметры:
  - `idx`: Индекс записи.

### Функция `prepare_data`

- `prepare_data(input_csv, output_dir, test_size=0.2)`: Подготавливает данные, разделяя их на обучающую и тестовую выборки. Принимает следующие параметры:
  - `input_csv`: Путь к входному CSV-файлу.
  - `output_dir`: Директория для сохранения обработанных данных.
  - `test_size`: Размер тестовой выборки (доля от общего объема данных). По умолчанию равен 0.2.

## Примеры использования

```python
# Инициализация токенизатора
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Чтение данных из CSV файла
data = pd.read_csv("data.csv")

# Создание объекта датасета
dataset = MedicalDataset(data, tokenizer, max_length=128)

# Подготовка данных
prepare_data("data.csv", "output_dir", test_size=0.2)
```