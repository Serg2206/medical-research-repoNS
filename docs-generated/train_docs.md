# Documentation: train.py

**Файл:** `train.py`

**Дата:** 2025-11-11

---

# Документация для кода

Этот код представляет собой пример загрузки, подготовки и использования данных для обучения модели классификации последовательностей на основе BERT с использованием библиотеки transformers.

## Описание функционала

1. **Настройка логирования**: Этот код настраивает логирование для отслеживания процесса обучения и отладки.

2. **Параметры модели**: Здесь задаются параметры модели, включая имя модели и устройство, на котором будет производиться обучение.

3. **Класс MedicalDataset**: Этот класс представляет собой настраиваемый датасет, который используется для загрузки и подготовки данных для обучения и тестирования модели.

4. **Функция prepare_data**: Эта функция загружает данные из CSV-файла, разделяет их на обучающую и тестовую выборки и сохраняет их в указанной директории.

## Параметры и возвращаемые значения

1. **Класс MedicalDataset**
    - `data`: DataFrame с данными для обучения или тестирования.
    - `tokenizer`: Токенизатор, используемый для преобразования текста в токены.
    - `max_length`: Максимальная длина последовательности токенов.
    
    Методы:
    - `__len__`: Возвращает количество примеров в датасете.
    - `__getitem__`: Возвращает токенизированный текст и метку для указанного индекса.

2. **Функция prepare_data**
    - `input_csv`: Путь к входному CSV-файлу.
    - `output_dir`: Директория для сохранения обработанных данных.
    - `test_size`: Размер тестовой выборки в долях от общего количества данных.

    Возвращает: None. Функция сохраняет обработанные данные в указанной директории.

## Примеры использования

```python
# Загрузка и подготовка данных
prepare_data("input.csv", "output_dir")

# Создание токенизатора
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Загрузка данных
data = pd.read_csv("output_dir/train.csv")

# Создание датасета
dataset = MedicalDataset(data, tokenizer)

# Получение примера данных
input_ids, attention_mask, label = dataset[0]
```