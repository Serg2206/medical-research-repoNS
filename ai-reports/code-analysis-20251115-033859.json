[
  {
    "file": "train.py",
    "timestamp": "2025-11-15T03:37:29.975817",
    "metrics": {
      "complexity": "train.py\n    F 77:0 train_model - A (4)\n    F 47:0 prepare_data - A (3)\n    C 22:0 MedicalDataset - A (2)\n    M 23:4 MedicalDataset.__init__ - A (1)\n    M 28:4 MedicalDataset.__len__ - A (1)\n    M 31:4 MedicalDataset.__getitem__ - A (1)\n",
      "maintainability": "train.py - A (69.54)\n"
    },
    "ai_recommendations": "В целом, код хорошо структурирован и читаем, но есть несколько мест, которые можно улучшить:\n\n1. **Hardcoding**: Имя модели и устройство, на котором будет происходить обучение, зашиты в коде. Это может быть неудобно, если вы хотите использовать другую модель или устройство. Рекомендуется вынести эти параметры в аргументы командной строки или конфигурационный файл.\n\n2. **Обработка ошибок**: В функции `prepare_data` есть проверка на наличие столбцов 'text' и 'label' в данных. Это хорошо, но можно добавить больше проверок, например, на то, что файл с данными существует и его можно прочитать.\n\n3. **Очистка данных**: В данном коде пример очистки данных очень простой - удаление пробелов в начале и конце текста. В зависимости от данных, возможно, потребуется более сложная предобработка.\n\n4. **Оптимизация обучения**: В цикле обучения после каждого шага градиент обнуляется. Это может быть неэффективно, если ваши данные не помещаются в память и вы используете `DataLoader` с параметром `num_workers > 0`. В этом случае рекомендуется использовать `optimizer.zero_grad(set_to_none=True)` для обнуления градиента.\n\n5. **Сохранение прогресса обучения**: В коде нет сохранения прогресса обучения. Можно добавить сохранение модели после каждой эпохи, чтобы в случае прерывания обучения можно было продолжить с последней эпохи.\n\n6. **Логирование**: Хотя логирование уже используется в коде, его можно улучшить, добавив больше информации о процессе обучения, например, значение функции потерь после каждого шага или батча.\n\n7. **Комментарии и документация**: Хотя большинство функций имеют комментарии, некоторые из них могут быть более подробными или точными. Например, в функции `prepare_data` можно указать, что она также сохраняет обработанные данные.",
    "lines_of_code": 124
  },
  {
    "file": "main.py",
    "timestamp": "2025-11-15T03:37:51.702012",
    "metrics": {
      "complexity": "main.py\n    F 32:0 train_model - A (4)\n    F 72:0 predict - A (2)\n    C 14:0 MedicalNet - A (2)\n    F 92:0 main - A (1)\n    M 15:4 MedicalNet.__init__ - A (1)\n    M 22:4 MedicalNet.forward - A (1)\n",
      "maintainability": "main.py - A (74.92)\n"
    },
    "ai_recommendations": "В целом, код хорошо структурирован и легко читаем. Однако есть несколько мест, которые можно улучшить:\n\n1. **Импорт библиотек**: В начале файла импортируется библиотека `os`, но она не используется в коде. Если она не нужна, ее следует удалить.\n\n2. **Документация**: Ваш код не содержит комментариев и документации, которые могут помочь другим разработчикам понять, что делает ваш код. Рекомендуется добавить docstrings к функциям и классам, а также комментарии к сложным или неочевидным участкам кода.\n\n3. **Обработка ошибок**: Ваш код не содержит обработки ошибок или проверок входных данных. Это может привести к неожиданным ошибкам во время выполнения. Например, вы можете добавить проверки, чтобы убедиться, что входные данные для обучения и прогнозирования имеют правильную форму.\n\n4. **Жесткое кодирование**: Ваш код содержит жестко закодированные значения, такие как размеры скрытых слоев и количество эпох. Это может сделать ваш код менее гибким для изменений. Рекомендуется сделать эти значения параметрами функций или классов, чтобы их можно было легко изменять.\n\n5. **Печать во время обучения**: Ваш код печатает потери после каждой эпохи. Это может быть полезно для отладки, но может замедлить обучение, если данных много. Рекомендуется добавить параметр, который позволяет включать или отключать печать во время обучения.\n\n6. **Сохранение модели**: Ваш код сохраняет модель после обучения, но не предоставляет возможности изменить имя файла сохранения. Рекомендуется сделать имя файла параметром функции обучения.\n\n7. **Производительность**: Ваш код использует CPU, если GPU недоступен. Это хорошая практика, но вы можете добавить сообщение, которое информирует пользователя, используется ли GPU или CPU. Это может помочь в отладке проблем с производительностью.\n\n8. **Разделение данных**: Ваш код не разделяет данные на обучающий и тестовый наборы. Это важно для оценки производительности модели. Рекомендуется добавить функцию для разделения данных.\n\n9. **Безопасность**: Ваш код загружает модель из файла без проверки, существует ли файл или нет. Это может привести к ошибке во время выполнения. Рекомендуется добавить проверку на существование файла перед его загрузкой.",
    "lines_of_code": 111
  },
  {
    "file": "infer.py",
    "timestamp": "2025-11-15T03:38:28.351273",
    "metrics": {
      "complexity": "infer.py\n    F 10:0 infer_model - A (2)\n",
      "maintainability": "infer.py - A (74.30)\n"
    },
    "ai_recommendations": "В целом, код хорошо написан и следует большинству best practices. Однако есть несколько областей, которые можно улучшить:\n\n1. **Производительность**: Ваша модель и токенизатор загружаются каждый раз, когда вызывается функция `infer_model`. Если вы планируете вызывать эту функцию несколько раз, это может быть неэффективно. Рассмотрите возможность загрузки модели и токенизатора один раз и передачи их в функцию в качестве аргументов.\n\n2. **Читаемость**: Ваш код в целом хорошо структурирован и легко читается. Однако, вы можете добавить больше комментариев для объяснения, что делает каждый блок кода, особенно для сложных или неочевидных частей.\n\n3. **Best Practices**: Ваш код уже следует большинству best practices, включая использование `argparse` для обработки аргументов командной строки и проверку `if __name__ == \"__main__\"` перед выполнением кода. Однако, вы можете рассмотреть возможность добавления обработки исключений для обработки возможных ошибок, таких как отсутствие модели по указанному пути или некорректный ввод текста.\n\n4. **Безопасность**: Ваш код не содержит очевидных проблем с безопасностью. Однако, если текст, который вы анализируете, может быть введен пользователем или получен из ненадежного источника, вы должны убедиться, что он корректно обрабатывается и не содержит вредоносного кода. В данном случае, токенизатор `transformers` должен корректно обрабатывать ввод.\n\nВот как может выглядеть обновленный код:\n\n```python\nimport argparse\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Настройка устройства\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef load_model(model_dir):\n    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n    model.to(DEVICE)\n    return tokenizer, model\n\ndef infer_model(model, tokenizer, text):\n    tokens = tokenizer(\n        text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128\n    )\n    tokens = {k: v.to(DEVICE) for k, v in tokens.items()}\n\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**tokens)\n        probabilities = torch.softmax(outputs.logits, dim=-1)\n\n    return probabilities.tolist()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Инференс с медицинской нейросетью\")\n    parser.add_argument(\"--model\", required=True, help=\"Путь к сохраненной модели\")\n    parser.add_argument(\"--text\", required=True, help=\"Текст для анализа\")\n    args = parser.parse_args()\n\n    tokenizer, model = load_model(args.model)\n    result = infer_model(model, tokenizer, args.text)\n    print(f\"Результаты предсказания: {result}\")\n```\n\nВ этом обновленном коде, модель и токенизатор загружаются один раз с помощью функции `load_model`, а затем передаются в функцию `infer_model` в качестве аргументов.",
    "lines_of_code": 35
  },
  {
    "file": "prepare_data.py",
    "timestamp": "2025-11-15T03:38:44.035787",
    "metrics": {
      "complexity": "prepare_data.py\n    F 13:0 prepare_data - A (3)\n",
      "maintainability": "prepare_data.py - A (84.99)\n"
    },
    "ai_recommendations": "Ваш код в целом хорошо написан и следует многим best practices. Однако, есть несколько мест, которые можно улучшить:\n\n1. **Обработка исключений**: Ваш код может вызвать исключение, если CSV-файл не существует или не содержит ожидаемых столбцов. Это хорошо, но можно добавить более детальное сообщение об ошибке, чтобы пользователь понимал, что именно пошло не так.\n\n2. **Очистка данных**: Вы используете `str.strip()` для очистки данных. Это хорошо, но может быть не достаточно в зависимости от вашего конкретного случая. Возможно, вам стоит добавить дополнительные шаги очистки, такие как преобразование текста в нижний регистр, удаление пунктуации и т.д.\n\n3. **Комментарии**: Ваш код хорошо документирован, но некоторые комментарии могут быть избыточными, так как код и так является достаточно понятным. Например, комментарий `# Разделение данных` можно убрать, так как из названия функции `train_test_split` и так понятно, что происходит разделение данных.\n\n4. **Захардкоженный `random_state`**: В функции `train_test_split` вы используете захардкоженное значение `random_state=42`. Это может быть нормально для ваших текущих целей, но в общем случае лучше делать `random_state` параметром функции, чтобы у пользователя была возможность контролировать случайность разделения данных.\n\n5. **Использование f-строк**: Вы используете f-строки для форматирования сообщений логирования, что является хорошей практикой. Однако, в некоторых случаях это может быть неэффективно, так как f-строка будет всегда вычисляться, даже если уровень логирования не позволяет выводить данное сообщение. Вместо этого можно использовать старый стиль форматирования строк в логировании: `logger.info(\"Обработанные данные сохранены в %s\", output_dir)`.",
    "lines_of_code": 53
  },
  {
    "file": "scripts/fix_csv_headers.py",
    "timestamp": "2025-11-15T03:38:59.002018",
    "metrics": {
      "complexity": "",
      "maintainability": "scripts/fix_csv_headers.py - A (90.78)\n"
    },
    "ai_recommendations": "В целом, код написан хорошо и легко читается. Однако есть несколько рекомендаций, которые могут помочь улучшить его:\n\n1. **Производительность**: Вместо того, чтобы сначала считывать все строки из файла, а затем фильтровать их, можно считывать и фильтровать строки по одной. Это сократит использование памяти, особенно для больших файлов.\n\n2. **Безопасность**: Ваш код перезаписывает исходный файл, что может быть опасно. Если что-то пойдет не так в процессе работы скрипта, вы можете потерять исходные данные. Рекомендуется записывать результаты в новый файл.\n\n3. **Читаемость**: Вместо того, чтобы проверять `\",\" in line` для определения, является ли строка действительной, можно использовать `csv` модуль для чтения и обработки CSV-данных. Это сделает код более надежным и понятным.\n\n4. **Best practices**: Вместо того, чтобы жестко кодировать имя файла, лучше сделать его аргументом командной строки. Это сделает ваш скрипт более гибким и повторно используемым.\n\nВот как может выглядеть улучшенная версия кода:\n\n```python\n#!/usr/bin/env python3\n\"\"\"Fix CSV file by removing invalid header rows\"\"\"\n\nimport csv\nimport sys\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\n# Read the CSV file and filter out invalid lines\nvalid_lines = []\nwith open(input_file, \"r\", encoding=\"utf-8\") as f:\n    reader = csv.reader(f)\n    for i, row in enumerate(reader):\n        if i == 0 or len(row) > 1:  # Keep the header row and any row with more than one field\n            valid_lines.append(row)\n        else:\n            print(f\"Removing invalid line {i+1}: {','.join(row)}\")\n\n# Write back the cleaned CSV\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    writer = csv.writer(f)\n    writer.writerows(valid_lines)\n\nprint(\n    f\"\\nDone! Kept {len(valid_lines)} valid lines, removed {i+1 - len(valid_lines)} invalid lines\"\n)\n```\n\nТеперь вы можете вызвать скрипт с двумя аргументами: именем входного файла и именем выходного файла.",
    "lines_of_code": 29
  }
]